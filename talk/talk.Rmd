```{r setup, include=FALSE}
opts_chunk$set(cache = TRUE)
opts_chunk$set(tidy = FALSE) # preserve wrapped code
opts_chunk$set(dev = 'pdf') # size plots better
opts_chunk$set(fig.cap = "")
require(plyr)
require(reshape2) # for 'tips' dataset
require(ggplot2)
require(grid) # for "unit" function for ggplot legend size
require(rdatamarket)
require(forecast)
require(XML)
require(xtable)
require(maps)
require(stinepack) # for ggplot equivalent of decompose
```

\note{\tiny{Hello, my name is Duncan Garmonsway.  I am an analyst in the Road Policing Group.  Before that I was an analyst at the New Zealand Defence Force, and before that I was an analyst at the Ministry of Justice.  So I know a little bit about analysis in the public sector, and that's what I'm going to talk to you about today.

You should also know that I'm a perfectionist, and that I'm extremely idle.  So I will explain how to do really good analysis with the least possible effort.

In the first part I'll make the case for improving the way we do our analysis, and in the second part I'll show that there's really nothing new to learn.  We are already all familiar with the basic concepts.  In the third part I'll show off some magical things that can be achieved with a proper analytical tool like R, and in the final section I'll reassure you that R isn't just some weird shareware I found online once.  It's cutting edge technology used by the big movers and shakers.

But to begin with something small.  Me, doing a typical first piece of work for a new employer.}}

# Why You Should Care

#### Redo This Thing
"Just redo this spreadsheet we already did, to answer this OIA."

\note{\tiny{Okay, that's good.  OIAs, they're in the eye of the media, and they're one of the fundemental elements of a free, democratic society, so it's probably all right if I mess this up a bit.  So I look at the spreadsheet that they already did that I'm going to copy.  And let's say I'm working for the Police at this point, because I've already found that the Police get things more right than some other agencies.  So I think, good, this piece of analysis is *reproducible*.  What do I mean?}}

### Reproducibility?
#### Record of the data
#### Record of the analysis
#### Record of the presentation
#### Record of who did it and when

\note{\tiny{All I have to do is look at the data and get the up-to-date version.  Then I look at how the analysis was done, and do it again on the new data.  Finally I look at how the analysis was presented, and I copy the relevent bits of data and analysis into a new version of the presentation and send it off.  So that was reproducible analysis, wasn't it?

Nope.  When I try to get the up-to-date data, I get slightly different numbers, because it turns out there was a little bit of tweaking done between the database and the spreadsheet.  And then a bit more tweaking done in the spreadsheet, saved over the top.

Then the analysis isn't quite consistent.  There's a column of formulae that I assume are all the same, except that one of the formulae part-way down is slightly different to cover a special case in the data.  And there's nothing to explain what the special case is, or how the formula handles it.  And anyway, the up-to-date data has a couple more special cases that need to be handled in the same way, which means I need to type that special formula in somewhere else, too.

Once I've sorted out all the data, and all the analysis, and I get to the presentation.  I copy the relevant bit from the analysis and paste it into the report, or the powerpoint, or whatever it is, and it doesn't fit.  It's too big now.  So I spend forever adjusting everything to fit.

Finally I send out the presentation, and straight away I get an email back that says, "Hey, that was the March presentation.  This is April".  What have I done?  I've forgotten to change the title at the top.  Or if it's going to Joe Public, Joe Public writes back to complain that, in order to exercise his democratic right to information, he has to buy spreadsheet software from an American business.}}

### Reproducibility
#### Accurate
#### Transparent
#### Efficient

\note{\tiny{So is that really reproducible analysis?  No.  Is it easy?  Well, sort of.  Is it accurate?  Most of the time.  Is it transparent?  To some extent.  Can anyone outside the immediate team understand it?  Probably not.  Can a idle person repeat it at a click of a button?  Definitely not.

Obviously there is room for improvement.  Improvement is no longer in the too hard basket.  Is it worth it?}}

### Impact
#### High volume
#### High impact
#### Achievable

\note{\tiny{Doing real reproducible analysis would impact on all our work, increase our credibility under scrutiny, and we can already do it.

Analysis is this team's bread and butter.  OIAs, information requests, papers, even our own team projects like investigating the integrity of BO Universes; these can all be done in a reproducible way.

Most of our work is read by important people like senior management, politicians, the media and the public.  They all want a piece of us.  We must get everything right.

If you've have used email today, you already know how to do reproducible analysis.  Emails and reproducible analysis are both based on one fundamental thing: the humble text file.  Text files are cheap, simple, and versatile.  They're not ones and zeros---they're human readable---yet they can do everything from storing data, to programming computers, to presenting information.  To prove it, this whole slideshow is in fact just a text file, written in R.

But you can do it too, because you can use email, and email is also only a text file.}}

# You Can Do It

## Text

### Text Files
\scriptsize{MIME-Version: 1.0\\
Received: by 10.141.132.10 with HTTP; Fri, 27 Feb 2013 22:18:46 -0800 (PST)\\
Date: Sat, 28 Feb 2009 19:18:46 +1300\\
Delivered-To: nacnudus@gmail.com\\
Message-ID: 646bfbfc0902272218k33f818a0n257983794298ae74@mail.gmail.com\\
Subject: A good joke\\
From: Duncan Garmonsway obfuscated1@gmail.com\\
To: Harold Garmonsway obfuscated2@gmail.com\\
Content-Type: multipart/alternative; boundary=000e0cd29660dd3ae20463f48edd\\
--000e0cd29660dd3ae20463f48edd\\
Content-Type: text/plain; charset=ISO-8859-1\\
Content-Transfer-Encoding: 7bit\\
\vspace*{2\baselineskip}}
\small{Shakespeare on Excel:
=OR(B2,NOT(B2))
more funnies:
div dir="ltr" a href="here" http://www.jokes.com/ a br /div
}

\note{\tiny{There's some gobbledegook, but if we were experts in emails we would be able to understand that.  It isn't ones and zeros.  It's human-readable text.  If the computer broke and all it gave us was this text, we could still read it.

How does it work?}}

### Text Files
* Instructions
  
\scriptsize{MIME-Version: 1.0\\
Received: by 10.141.132.10 with HTTP; Fri, 27 Feb 2013 22:18:46 -0800 (PST)\\
Date: Sat, 28 Feb 2009 19:18:46 +1300\\
Delivered-To: nacnudus@gmail.com\\
Message-ID: 646bfbfc0902272218k33f818a0n257983794298ae74@mail.gmail.com\\
Subject: A good joke\\
From: Duncan Garmonsway obfuscated1@gmail.com\\
To: Harold Garmonsway obfuscated2@gmail.com\\
Content-Type: multipart/alternative; boundary=000e0cd29660dd3ae20463f48edd\\
--000e0cd29660dd3ae20463f48edd\\
Content-Type: text/plain; charset=ISO-8859-1\\
Content-Transfer-Encoding: 7bit\\
\vspace*{2\baselineskip}}

* Database
  
\small{Shakespeare on Excel:
=OR(B2,NOT(B2))
more funnies:
div dir="ltr" a href="here" http://www.jokes.com/ a br /div
}

\note{\tiny{This text is a database, a computer program, and is presentation-ready.

At the core of this email is a database.  It doesn't look like a database because it doesn't cost \$gazillions and because humans can read it.  The database is simply the information that I want my brother to know.  It's the body of the email.

The top section is a set of instructions to the computer to tell it what to do.  The instructions say to send a message from me to my brother, at this time, conveying this information.  Both computers and humans can read it.  Humans can write it.  It isn't a complete mystery, and most of the time we can forget it's there.  Someone invented it once, and now we can all use it without thinking about it.  But if it broke, or we wanted to tinker with it, we could understand it relatively easily.

And it's presentation-ready.  You can decide how you want to read this email.  He can read it in his own Gmail account.  He can forward it to his work and read it in Outlook.  He can read it on his phone.  He can share it with his blind friend who can read it in Braille.  Whatever he decides to do, I don't care, because it doesn't imply any more work for me.  I don't have to rewrite the email in Braille.

It's also accurate, transparent and efficient.  Emails rarely make mistakes.  You're not tied down to a particular software vendor.

That's all very well for emails, but what about our work?  You want to see a text file draw a graph.}}

### Plot
```{r simplePlot, warning = FALSE, fig.width = 4, fig.height = 3}
plot(x = c(1, 2, 3, 4, 5),
     y = c(10, 20, 15, 8, 2), type = "line")
```

\note{\tiny{This is a text file - just one line of text - that draws the graph.  It's a very rudimentary graph for the sake of time, but it illustrates the simplicity of R.  It could be made more attractive, just as graphs in Excel can be made more attractive.  Everything I've talked about is there.  There's the database.  There's the instruction.  The instruction is human-readable---everyone knows it's doing a line chart of these numbers against these axis.  It's accurate---the computer isn't going to make a mistake by accidentally missing off the bottom row of data in the spreadsheet.  And it's presentation-ready.  It so happens that that actual line of code drew this actual graph that's projected now, but it didn't have to be.  It could be printed instead, or it could go on a webpage.  It would be the same code, and in fact it would be the same software.  The 'R Stack' this talk is about did all the leg work.  It read the instruction, drew the plot, and put it on a slide.  If I was going to send it to Joe Public, then Joe public could obtain the software for free, or at the very least he could see what I'd done just by reading the instructions.

So we've done it?  End of presentation?  Not yet.  Now that we're using text, let's see what else we can do.

What if I tweaked my presentation a bit for next time.  What if I changed one of the numbers in the plot?  If I were using Powerpoint and Excel then I'd redraw the plot in Excel, copy it over the top of the old one in Powerpoint, and save it as My Presentation V2.ppt.  But there's no record of what it was I just did.  There's no audit trail.  I know what you're thinking, you're thinking, what about track changes?.  Um, show me where that button is on the Excel ribbon.}}

### Where's the Error?
\includegraphics[width=4in]{figures/ExcelError.png}

\note{\tiny{Tracked changes doesn't work on spreadsheets.  It works really well on text files.}}

## Version Control

### Diff
\includegraphics[width=4in]{figures/diff.png}

\note{\tiny{Here's some of the code that produces this actual slideshow.  It shows 'diff' between two versions of the actual code, before and after I fixed a couple of my mistakes.  It's a trail of my incompetance.  The red lines were deleted.  Green lines were added.

Looking at the middle pair, you can see that effectively the red line was re-written.  It was deleted, and a very similar line has been inserted in its place.  There are two differences between them, the first is labelled this section of code, changing its label from "meanRbiByYear" to "basicPlot", which made more sense to me at the time.  The other is that I've increased the height of the plot from 2.25 inches to 2.5 inches.

Along with that change, I will have recorded what it was I'd done.  There's a whole history of this presentation that records all the changes to it, when they were made, who made them, and what they said they'd changed.  This is the audit trail.  When we report a funny number and someone doubts us (how dare they), we need to have at hand the exact steps that got us from the data to the report.  The report might have been produced several months ago, and since then we might have changed a few things, so we need to be able to recover every version of every analysis that we do.

If you're now thinking, "That's great for text files, but what if the data folder goes somewhere else? All my linked data sources will be broken", then you're in luck, because this kind of track changes also tracks directories.}}

### Folder Structure
\includegraphics[width=4in]{figures/folderTree.png}

\note{\tiny{That's cute, but it's kinda weird.  Where did you dig that up, Duncan?  Well, Facebook uses version control.  So does Google.  In fact, everybody uses it.  Nobody uses the "Last modified" timestamp on a file called "Presentation for Dave - Megan's comments - v4 draft.ppt"

The reason is collaboration.  Version control uncomplicates collaboration.  Collaboration is another thing we do every day, and we could do more of every day.  Imagine if Megan didn't have to manually merge our individual Weekly Reports into one big Weekly Report?  Effectively, we're all contributing to the same document, so why don't we all use the same file and work on it at the same time?  Because it's a recipe for spaghetti.  But version control would make that possible.  Version control uses a special brand of spaghetti that all lies neatly parallel and never collides.}}

### Diff Tree
\includegraphics[width=4in]{figures/diffTree.png}

\note{\tiny{This is several people collaborating on one document, without their individual workstreams ever colliding.  It's like a very safe railway network where all the tracks run parallel, except at carefully controlled junctions.  This is called a trunk, this is called a branch, and the whole thing is called a tree.

This is the trunk.  The main document.  People are contributing to their own copies of that document, which are the other coloured lines, the branches.  From time to time, they merge their changes into the main document.

So we've covered the fundamental component of reproducible analysis, which is the text file.  It records the data, it records the analysis, and it's presentation-ready.  It's easy to exchange, it's easy to collaborate, it's cheap, and we already know how to use it.  That OIA I'm writing is on the right track.  The next piece of the puzzle is a good workflow.}}

# Do The Magic

## Workflow

### LCFD

#### Least Commonly Fouled-up Data

#### Load
#### Clean
#### Function
#### Do

\note{\tiny{Text is great, but it's still possible to stuff up the OIA.  Police already does this bit pretty well, so instead of spending a lot of time explaining it, I'll just give examples of how to implement this process using the R statistical programming language.

Basically, it's wise to use a modular process.  Divide it up into manageable chunks.  If the source of the data changes, all you should have to do is load the data in a different way.  You shouldn't have to go through all your charts again, selecting the new cells for the source data.  If the data acquires a couple of new inconsistencies or is available in a different form, all you should have to do is clean it in a different way.  You won't have to purge your pivot tables of any extra values that you don't need, because your pivot tables will never see any extra values, because you've already cleaned them out earlier.  You can be more efficient by keeping your natty little formulae and routines separately where you can use them for other pieces of work, and that improves accuracy too---if you fix a bug in one function, it's fixed for everybody at the same time.  Finally, the "Do", the actual analysis, is uncluttered by any tedious administrative stuff.  You'll be able to draw a graph a different way without having to change any of your loading or cleaning processes.

I'll quickly show you how easy loading and cleaning is using R, and then I'll show you some pretty pictures of what R can do that you can't do in Excel.}}

### Load
#### From a text file
`mydata = read.table("mydata.txt")`

#### From a database
`mydata <- sqldf("SELECT * FROM School", dbname = "Test2.sqlite")`

#### From the web
`dminit("4b3320aa36214e35a7d0c6c175a1ff98")`

`mydata <- dmlist("22ry")`

#### From other statistical packages
`mydata <- read.xls("mydata.xls")`

`mydata <- read.spss("myfile")`

\note{\tiny{You can load data into R from anywhere.  There are no compatibility problems.  If you want to use a Twitter feed, you can do that.  More likely, if you want a real-time traffic feed from the NZTA, you can use that to.  Heaven forfend that you have to use Excel, but you can even load data from spreadsheets.

The database connections understand SQL, Structured Query Language, which is a language specifically for retrieving data from databases.  When we click and drag stuff in Business Objects to create a query, behind the scenes it is writing SQL to peform that query.  Writing SQL ourselves gives us more control.

Incidentally, nobody knows how to pronounce SQL because everyone interacts via emails and Facebook these days, but I've heard it called "sequel".}}

### Clean
`x <- read.csv("data.csv")`

      COUNT HOUR     WEEKEND
          1    1 Not weekend
          2    1 Not weekend
          3    1 Not weekend
          2    1 Not weekend
          3    1     Weekend
          1    1     Weekend

\note{\tiny{This is the first six rows of a much larger dataset that I pulled from CAS.  I have an email from the manager of CAS saying that I'm allowed to freely redistribute this data, so it's okay.  I pulled a load of crashes, by hour of the day, and by weekday or weekend.  So the top four rows show that there were 1 plus 2 plus 3 plus another 2 crashes between midnight and 0100 hours on weekdays.

By the way, this is how you view data in R.  You type the name of the variable where the data is stored, and it prints it to the screen.  It's not interactive the way that Excel is---you can't click on a cell and change it, but then you can't click on a cell and change it accidentally.  That's just one of many benefits of being freed from the grips of the Excel grid, and if you're interested I can demonstrate some more advantages back at my desk.

That data's okay, but it's not useful to me yet.  I want to clean, or reorganise it a bit to be useful to me.  I don't want to have to add up the first four rows in my head, I want a table that says 1am, Not Weekend, 8 crashes.  2am, Not Weekend, five crashes.  And the same for weekends.

In Excel, I'd have to draw up a skeleton of the table that I wanted, and then I'd have to write a really complicated SUMIFS formula, and put that formula into every cell that I thought might return some crashes.  If I had more than about 1000 crashes, it would take ages to calculate.  The alternative would be a pivot table, and we all know what a pain it is to construct and maintain pivot tables.  But in R, it's one command.}}

### Clean
```
y <- dcast(x, hour ~ weekend,
  sum, value.var = "count")
```

          HOUR NOT WEEKEND WEEKEND
             1           4       6
             2           5       6
             3           4       5
             4           1       6
             5           5       5
             6           7       5

\note{\tiny{People say R is hard.  Not really.  But we can make it even easier.  If you find yourself writing that formula a lot, for lots of different crash lists, then you can make a shortcut, called a function.  Instead of saying, "follow these complicated instructions" every time, you say, "do that thing again".}}

### Function
```
myFunction <- function(z){
 dcast(z, hour ~ weekend, sum, value.var = "count")
}

y <- myFunction(x)
a <- myFunction(b)
```

\note{\tiny{You can do the same with graphs---draw one really fancy graph, and then easily duplicate it for as many datasets as you like.  This is a massive timesaver when you do a piece of work for one district, and suddenly all the districts want one, and it's also good for accuracy.  You won't mistype that formula, because you won't have to type it again.}}

## Magic

### Facet
1.  Give me the number of crashes by the hour of the day.
1.  Also by whether it's the weekend.
1.  Also by urban/rural.
1.  Also by drink/drug or not.

\note{\tiny{This is where the magic starts.  Now we're doing the analysis.  This graph answers the kind of question we often get, especially in OIAs.  It's called multi-dimensional, where the dimensions are hour of day, weekday/weekend, urban/rural, drink/drug.  Excel makes this job incredibly tedious, because you have to analyse each pair of dimensions separately.  You could keep on adding dimensions to a pivot chart, but it quickly becomes unreadable.  Here, R does all the dimensions in one go.}}

### Facet
```{r bayOfPlenty, echo = FALSE, fig.width = 4, fig.height = 3}
x <- read.csv("BoP.csv", header = TRUE)
colnames(x) <- c("count", "hour", "weekend", "fatal", "alcohol", "urban")
x$count <- as.numeric(x$count)
x$hour <- as.numeric(x$hour)
# abbreviate alcohol column for graph labels
alc <- factor(c("alc/drug", "clean"))
x$alcohol <- alc[x$alcohol]
y <- dcast(x, hour + weekend + fatal + alcohol + urban ~ ., fun.aggregate = sum, value.var = "count")
colnames(y)[6] <- "count"
y <- y[rep(row.names(y), y$count), 1:5]
ggplot(y, aes(hour, group = weekend, fill = weekend)) + 
  geom_density(alpha = 0.2) + 
  facet_grid(alcohol ~ urban, margins = TRUE) +
  theme(legend.position="bottom", legend.title=element_blank()) +
  theme(legend.key.size = unit("0.2", "cm")) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
```

\note{\tiny{Each of those graphs was a single command.  One line of code.

Okay, but you could almost do that in Excel.  What about the complex statistics and forecasting that Nils is going to be doing.  Can Excel keep up with that?}}

### Forecast
```{r gas, echo = FALSE, fig.width = 4, fig.height = 3.5}
plot(forecast(gas), main = "Australian monthly gas production")
```

\note{\tiny{The black line is the production of gas per month in Australia.  The blue line is the forecast.  Note that the trend is seasonal---it goes up and down regularly on an annual cycle.  So does the forecast.  To see how it does it, we can plot the decomposition.}}

### Decompose
```{r gasDecompose, echo = FALSE, fig.width = 4, fig.height = 3}
# function for ggplot equivalent of decompose on a timeseries
decomp <- function(series,frequency){ 
    x <- na.stinterp(series)  #this is for interpolation 
    x1 <- ts(x,start=01,freq=frequency) #converting into a ts object
    x2 <- stl(x1,s.window="periodic") #performing stl on the ts object
    x3 <- data.frame(x,x2$time.series) #creating a data.frame
    x3$index <- index(x3) #create an id 
    colnames(x3)[1] <- "time-series"  
    x4 <- melt(x3,id.vars="index")   #melt the dataframe into long format  
    ggplot(x4,aes(index,value,group=variable)) + geom_line(alpha=0.7) + facet_grid(variable ~., scales="free")  + xlab("Timesteps") + ylab("Values") + theme(axis.text = element_blank(), axis.ticks = element_blank())
}
decomp(as.numeric(gas), 12)
```

\note{\tiny{R has decomposed the whole series into three components - the regular seasonal cycle, the upward trend as production of gas increases, and the random fluctuations left over.  If we were reporting these figures, we could forget about the top three graphs, because they're boring and predictable.  The management would want to know about the bottom graph, the unpredictable one.

This kind of functionality is typical of R.  It doesn't just do stuff by magic.  It isn't opaque.  It allows you to know what's going on behind the scenes.  That graph was also a single command.

But we rarely have such clearly seasonal data in Road Policing.  What use is such sophisticated forecasting to us?}}

### Forecast (harder)
```{r setupBeer, echo = FALSE}
# beer production
dminit("4b3320aa36214e35a7d0c6c175a1ff98") # rdatamarket api key
beer <- dmlist("22ry") # download
beer$Year <- as.numeric(substr(as.character(beer$Quarter), 1, 4)) # extract year from quarter
annual <- ddply(beer[1:152, ], .(Year), summarize, Production = sum(Value)) # summarize by year
# forecast with forecast package
goodForecast <- forecast(as.ts(annual[1:34, "Production"]), h = 4)
goodConfidence <- data.frame(goodForecast$upper, goodForecast$lower)
goodForecast <- data.frame(Year = seq(1990, 1993), Production = goodForecast$mean) # df for ggplot
# forecast with simple regression
production <- annual[1:34, "Production"]
year <- annual[1:34, "Year"]
poorForecast <- lm(production ~ year) 
pred <- predict(poorForecast, data.frame(year = seq(1990, 1993))) # four-year forecast
poorForecast <- data.frame(Year = c(poorForecast$model$year, seq(1990, 1993))
                           , Production <- c(poorForecast$fitted.values, pred))
# preapre plot (plain data to begin with)
p <- ggplot() + xlim(range(annual$Year)) + 
  ylim(range(c(annual$Production, goodForecast$Production, poorForecast$Production, goodConfidence$X2))) + 
  geom_line(data = annual[1:34, ], aes(Year, Production)) + 
  theme(legend.position="none")
```

```{r BeerInvisible, echo = FALSE, fig.width = 4, fig.height = 3}
p
```

\note{\tiny{This is more like the graphs we have to forecast.  Can anyone guess what this is?}}

### Forecast (harder)
```{r BeerVisible, echo = FALSE, fig.width = 4, fig.height = 3}
p <- p + labs(title="Australian Annual Beer Production")
p
```

\note{\tiny{That's right, it's the Australian annual beer production.  Let's show how Excel would forecast this.}}

### Forecast (harder)
```{r BeerForecast1, echo = FALSE, fig.width = 4, fig.height = 3}
p <- p + geom_line(aes(Year, Production),  poorForecast, colour = "red")
p
```


\note{\tiny{It's done a linear regression, and it's terrible.  Who wants to send that upstairs to the boss?  So let's see whether a sophisticated forecasting package can do any better.}}

### Forecast (harder)
```{r BeerForecast2, echo = FALSE, fig.width = 4, fig.height = 3}
p <- p + geom_line(aes(Year, Production), goodForecast, colour = "blue") +
  geom_ribbon(data=goodConfidence,aes(x = seq(1990, 1993), ymax = X1, ymin=X1.1), alpha=0.2)
p
```

\note{\tiny{Okay, it's drawn a fairly straight line, in blue, and it's given us confidence intervals.  It's 95\% confident that production will be somewhere within the grey area for the next four years.  How did it do?}}

### Forecast (harder)
```{r BeerForecast3, echo = FALSE, fig.width = 4, fig.height = 3}
p <- p + geom_line(aes(Year, Production), annual[34:38, ], colour = "darkgreen")
p
```

\note{\tiny{It got it 95% correct. 

Now I know you're still not impressed, and you won't be until you see an interactive chart that you can click on and that will sing you a song.  Unfortunately, the computer in this conference room doesn't have Chrome, so it can't show you that.  So instead, I'll show you just how easy it is to draw a map.}}

### Map
```{r map, fig.width = 4, fig.height = 2.8}
map("nz", mar = c(0,0,0,0))
```

\note{\tiny{There you go.  And guess what?  Having done all that analysis, it's a trivial step to publish it on paper, as a slideshow, or on the web.  I built this whole slideshow using R.  I never had to copy a graph from Excel and paste it into Powerpoint, and then do it again when I tweaked the chart a bit.  I never, for example, had to programme this menu bar on the right.  That just updates itself based on my slide headings.

I hope by this stage you have three ideas in your heads.  First, you want to benefit from reproducible research.  Second, you feel confident you can do it, because it's only text files so it's no more difficult than an email.  Third, you're impressed by what R can do for you.  I suspect you also have a nagging doubt.  Is R reliable?}}

# What R is Anyway

### Origins
* University of Auckland
* First developed 1994
* Open source

\note{\tiny{R is the only statistical programming language invented by a Maori.  It's called R after it's co-inventors, Ross Ihaka and Robert Gentleman, and because it derives from an old language called S, for statistics.  Ross and Robert are/were professors of statistcs at Auckland University, and the reason they invented R was because existing software companies couldn't keep up with statistics research.  It's still the case that new statistical methods are implemented in R before any other software, usually because it's researched using R in the first place.  The forecast package I showed you earlier is a prime example.  Rob Hyndman has just published at an international conference his research into heirarchichal forecasting, finding a method that's better than either top-down, bottom-up or middle-out forecasting

The first version of R was complete in 1994, compared with about 1972 for SAS, and 1985 for Excel.  Since then R's ongoing development has been driven by leading professors of statistics and software around the world, and its rate of continual improvement is only increasing.

Because R is open source, and because it's extensible---any user can write a new function for R and publish it---the more users R is getting, the faster it is being developed, which is why R development is increasing exponentially.  If the last time you used R was a couple of years ago, then you really need to look at it again, because the improvements in functionality, useability, and connectivity with the web will blow you away.  

The fact that it's open source means several good things.  Firstly, it's free to use.  It's free to extend, so there is literally no function that R lacks, because you can write it yourself.  Finally, it attracts the best developers, who are academics.  They're the best because they're not interested in profit, they're only interested in analysing data.}}

### Developers
```{r echo = FALSE, results='asis'}
x <- read.csv("maintainersShort.txt", header = FALSE)
colnames(x) <- c("Maintainer", "Position", "University")
print(xtable(x), size = "\\tiny", comment = FALSE)
```

\note{\tiny{R is as reliable as its developers, and R's developers are experts.  }}

### Users (support)
"The third problem in your code is, you are printing plots from R base graphics. Sigh. Printing base graphics does not make any sense."

\note{\tiny{R is also as reliable as its users.  If I make a mistake, that's my fault.  But R has a huge support network that will catch me when I fall.  I got a bit stuck preparing one of the charts in this presentation, so I posted my problem online.  Within half an hour, I had this response.

So some random stranger abused me on the internet?  What's that to boast of?  Well, he's no random stranger.  That post was written to me by the package developer.  It would be like the chief developer of Microsoft Excel emailing me a solution to my problem, within half an hour, from a different country.  In fact, it's better than that, because Yihui Xie isn't just some random developer either, he's on the previous slide.  He's a postdoc in statistics at Iowa State University.  He knows what he's talking about.  If I'd happened to find a bug, then I could send the bug directly to the developer.  Compare that with Excel where you won't get a bug fix until ICT installs the latest version, which will be a long wait.

Okay, so R is reliable, there's incredible support available for it, but you've never heard of it.  You don't want to invest anything into some shareware thing, even though you now understand that everything's a text file so you can always get out of it, you won't be tied into propriety software the way you are with Excel and BO.  Is anyone else using R, and is it going anywhere?}}

### Pace of Development
```{r packages, echo = FALSE, fig.width = 4, fig.height = 3}
# adapted from http://blog.revolutionanalytics.com/2013/05/how-r-grows.html for packages and maintainers
url <- "http://cran.r-project.org/web/packages/available_packages_by_date.html"
page <- readHTMLTable(url, stringsAsFactors = FALSE)[[1]]
names(page) <- c("Date","Package","Title")
page$Year <- as.factor(substr(page$Date,1,4))
# plot
cbbPalette <- c(rep("#E69F00",3), "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
barP <- ggplot(page, aes(factor(Year)))
barP + geom_bar(fill=cbbPalette) +
  labs(title = "Available Packages") +
  ylab("Number") +
  xlab("Year of Last Update")
```

\note{\tiny{The pace of development of R is exploding.  That's one reason why businesses like ANZ use it.  And because it's open source, everyone will benefit from everybody-else's investment in it, so if ANZ develops a new feature in R, everyone we get to use it too.  Development is so fast, that there's almost nothing in R that hasn't been updated in the last couple of years.

This graph is actually slightly misleading, and I can use that to illustrate a couple of the great things about R that I've been talking about.  I've adapted this graph from its original publication on someone's blog.  One change I made was to use up-to-date data.  It didn't cost me any effort to do that, because the original analyst had written the code to download new data every time.  That's the first step in LFCD, and as you can see, it's really helpful.

The major change I made was to label the x-axis "Year of Last Update".  Without that label, you might think that the number of packages available was increasing exponentially.  That might be true, but the graph doesn't show that.  However, the blog where this graph was published interpreted it that way.  Fortunately, because they made their code available to, as a text file, I was able to work out exactly what was going on and correct it.}}

### The Competition
\includegraphics[width=4in]{figures/jobgraph.png}

\note{\tiny{This chart is a guestimate of jobs advertised for R versus the proprietary industry standard, SAS.  I won't read too much into this, except to point out that SAS has been having a tough couple of years, whereas R has never had it better.  It's surprising that SAS is trending down, given that the last couple of years has seen the rise of terms like "big data" and "data science" in the business world.  "Data science" is a fancy term for data analysis with connotations of doing the analysis better than we usually do.  "Big data" refers to, surprisingly, very large amounts of data, usually sales data or website clicks data---much larger datasets than the Police uses.  Now, SAS salespeople tend to claim that R can't handle big data, whereas SAS can.  If that were the case, you'd think that SAS would be doing really well now that big data is fashionable and available.  And you'd expect that R wouldn't be making any gains.}}

### The Competition
\includegraphics[width=4in]{figures/jobgraph2.png}

\note{\tiny{Out of interest, I've included SPSS as well.  That's the package that Nils is interested in, and it's one of the main proprietary competitors of SAS.}}

### Thank You
\begin{center}
\LARGE{Questions?}
\end{center}

\note{\tiny{How did I find out about R?
How long have I been using R?
How did I study R?
Steep learning curve?
Support from ICT?
Training?---Police-specific user guide?
Migrate slowly over (demonstrate an OIA in Excel and R?)
More time-consuming initially but save and gain in the long run}}